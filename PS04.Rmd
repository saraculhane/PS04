---
title: 'STAT/MATH 495: Problem Set 04'
author: "Sara Culhane"
date: '2017-10-03'
output:
  html_document:
    collapsed: no
    smooth_scroll: no
    toc: yes
    toc_depth: 2
    toc_float: yes
  pdf_document:
    toc: yes
    toc_depth: '2'
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE, fig.width=8, fig.height=4.5, message=FALSE)
set.seed(76)
```

# Collaboration

Please indicate who you collaborated with on this assignment: N/A


# Load packages, data, model formulas

```{r, warning=FALSE}
library(tidyverse)
library(mosaic)
credit <- read_csv("http://www-bcf.usc.edu/~gareth/ISL/Credit.csv") %>%
  select(-X1) %>%
  mutate(ID = 1:n()) %>% 
  select(ID, Balance, Income, Limit, Rating, Age, Cards, Education)
```

You will train the following 7 models on `credit_train`...

```{r}
model <- list() # Store all 7 models in a list
model[[1]] <- as.formula("Balance ~ 1")
model[[2]] <- as.formula("Balance ~ Income")
model[[3]]<- as.formula("Balance ~ Income + Limit")
model[[4]] <- as.formula("Balance ~ Income + Limit + Rating")
model[[5]] <- as.formula("Balance ~ Income + Limit + Rating + Age")
model[[6]]<- as.formula("Balance ~ Income + Limit + Rating + Age + Cards")
model[[7]] <- as.formula("Balance ~ Income + Limit + Rating + Age + Cards + Education")
```

... where `credit_train` is defined below, along with `credit_test`.

```{r}
set.seed(79)
credit_train <- credit %>% 
  sample_n(20)
credit_test <- credit %>% 
  anti_join(credit_train, by="ID")
```


```{r}
RMSE <- function(x) { # simple function for RMSE
  r <- sqrt(mean(x^2))
  return(r)
}
```

```{r}
RMSE_train <- runif(n=7)
RMSE_test <- runif(n=7)
RMSE_total <- data.frame(num_coefficients = 1:7, RMSE_test, RMSE_train) # Create data frame to store values since we can only return 1 in a function

f <- function(list)  {
  for (i in 1:7) { # predict and calcualte RMSE for all 7 models
    RMSE_test[i] <- RMSE(predict(lm(list[[i]],data=credit_train),credit_test) - credit_test$Balance) # Store test RMSE
    RMSE_train[i] <- RMSE(predict(lm(list[[i]],data=credit_train),credit_train) - credit_train$Balance) # Store train RMSE 
    RMSE_total[i,2] <- RMSE_test[i] # Add to data frame
    RMSE_total[i,3] <- RMSE_train[i]
  }
  return(RMSE_total) # return data frame
}
results <- f(model) # Store results in data frame


```



# RMSE vs number of coefficients







```{r}

# Some cleaning of results
results <- results %>% 
  # More intuitive names:
  rename(
    `Training data` = RMSE_train,
    `Test data` = RMSE_test
  ) %>% 
  # Convert results data frame to "tidy" data format i.e. long format, so that we
  # can ggplot it
  gather(type, RMSE, -num_coefficients)

ggplot(results, aes(x=num_coefficients, y=RMSE, col=type)) +
  geom_line() + 
  labs(x="# of coefficients", y="RMSE", col="Data used to evaluate \nperformance of fitted model")
```


# Interpret the graph

We see that the test data has a higher RMSE for all predictors when compared to our training set, which makes sense given that the train is being testing on the same data.  Howewever, we do see a fairly similar level of performance for both the 3 and 4 coefficient models (the model with 3 predictors seems to be the optimal model from a RMSE and simplification stand point).  After adding a 5th, the test set performs worse and the training data performs "better" in terms of RMSE.  However, at 5 predictors, the train predicts are doing well as a result of overfitting.



# Bonus

Repeat the whole process, but let `credit_train` be a random sample of size 380
from `credit` instead of 20. Now compare and contrast this graph with the
one above and hypothesize as to the root cause of any differences.

```{r, echo=FALSE}
set.seed(79)
credit_train <- credit %>% 
  sample_n(380)
credit_test <- credit %>% 
  anti_join(credit_train, by="ID")
```

```{r}
results <- f(model)
```


```{r}
# Some cleaning of results
results <- results %>% 
  # More intuitive names:
  rename(
    `Training data` = RMSE_train,
    `Test data` = RMSE_test
  ) %>% 
  # Convert results data frame to "tidy" data format i.e. long format, so that we
  # can ggplot it
  gather(type, RMSE, -num_coefficients)

ggplot(results, aes(x=num_coefficients, y=RMSE, col=type)) +
  geom_line() + 
  labs(x="# of coefficients", y="RMSE", col="Data used to evaluate \nperformance of fitted model")
```

Here we have a smaller gap between the train and test set RMSE, as the train (though still evaluated on known info) and test predictions are built on a much more robust training set than before (380 v. 20 points).  

Although this seems positive for our test predictions, we start to see issues with overfitting because of the small size of the test set around the 4th predictor added. At that point, the RMSE of the test set becomes lower than our training, indicating too close of a fit.
