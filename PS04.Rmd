---
title: 'STAT/MATH 495: Problem Set 04'
author: "Sara Culhane"
date: '2017-10-03'
output:
  html_document:
    collapsed: no
    smooth_scroll: no
    toc: yes
    toc_depth: 2
    toc_float: yes
  pdf_document:
    toc: yes
    toc_depth: '2'
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE, fig.width=8, fig.height=4.5, message=FALSE)
set.seed(76)
```

# Collaboration

Please indicate who you collaborated with on this assignment: N/A


# Load packages, data, model formulas

```{r, warning=FALSE}
library(tidyverse)
library(mosaic)
credit <- read_csv("http://www-bcf.usc.edu/~gareth/ISL/Credit.csv") %>%
  select(-X1) %>%
  mutate(ID = 1:n()) %>% 
  select(ID, Balance, Income, Limit, Rating, Age, Cards, Education)
```

You will train the following 7 models on `credit_train`...

```{r}
model <- list() # Store all 7 models in a list
model[[1]] <- as.formula("Balance ~ 1")
model[[2]] <- as.formula("Balance ~ Income")
model[[3]]<- as.formula("Balance ~ Income + Limit")
model[[4]] <- as.formula("Balance ~ Income + Limit + Rating")
model[[5]] <- as.formula("Balance ~ Income + Limit + Rating + Age")
model[[6]]<- as.formula("Balance ~ Income + Limit + Rating + Age + Cards")
model[[7]] <- as.formula("Balance ~ Income + Limit + Rating + Age + Cards + Education")
```

... where `credit_train` is defined below, along with `credit_test`.

```{r}
set.seed(79)
credit_train <- credit %>% 
  sample_n(20)
credit_test <- credit %>% 
  anti_join(credit_train, by="ID")
```


```{r}
RMSE <- function(x) { # simple function for RMSE
  r <- sqrt(mean(x)^2)
  return(r)
}
```

```{r}
RMSE_train <- runif(n=7)
RMSE_test <- runif(n=7)
RMSE_total <- data.frame(num_coefficients = 1:7, RMSE_test, RMSE_train) # Create data frame to store values since we can only return 1 in a function

f <- function(list)  {
  for (i in 1:7) { # predict and calcualte RMSE for all 7 models
    RMSE_test[i] <- RMSE(predict(lm(list[[i]],data=credit_train),credit_test)) # Store test RMSE
    RMSE_train[i] <- RMSE(predict(lm(list[[i]],data=credit_train),credit_train)) # Store train RMSE 
    RMSE_total[i,2] <- RMSE_test[i] # Add to data frame
    RMSE_total[i,3] <- RMSE_train[i]
  }
  return(RMSE_total) # return data frame
}
results <- f(model) # Store results in data frame


```



# RMSE vs number of coefficients







```{r}

# Some cleaning of results
results <- results %>% 
  # More intuitive names:
  rename(
    `Training data` = RMSE_train,
    `Test data` = RMSE_test
  ) %>% 
  # Convert results data frame to "tidy" data format i.e. long format, so that we
  # can ggplot it
  gather(type, RMSE, -num_coefficients)

ggplot(results, aes(x=num_coefficients, y=RMSE, col=type)) +
  geom_line() + 
  labs(x="# of coefficients", y="RMSE", col="Data used to evaluate \nperformance of fitted model")
```


# Interpret the graph

The test data performs at a lower RMSE only for the first two predictors.  After that, the RMSE test data grows much more rapidly, while the training data RMSE stays the same regardless of predictors added. Since our train data comes from such a small smaple, we have much more variability from the fitted model as we add predictors.This causes the much larger test dataset to perform poorly.

For the train set, overfitting examines a low RMSE despite poor predictive performance on the test set. Since we are fitting the model same data that we trained with for the training data, we are seeing the model flex to fit the original data very closely.

The reason we see a constant RMSE for predicting train on training data is because it will always make prediction values based on the same observed values, which will result in the same RMSE because the variability of the data will not change regardless of which model we are fitting. In other words, we are always placing a line about the same set of points that we have explicitly geneated generated that same line with.



# Bonus

Repeat the whole process, but let `credit_train` be a random sample of size 380
from `credit` instead of 20. Now compare and contrast this graph with the
one above and hypothesize as to the root cause of any differences.

```{r, echo=FALSE}
set.seed(79)
credit_train <- credit %>% 
  sample_n(380)
credit_test <- credit %>% 
  anti_join(credit_train, by="ID")
```

```{r}
results <- f(model)
```


```{r}
# Some cleaning of results
results <- results %>% 
  # More intuitive names:
  rename(
    `Training data` = RMSE_train,
    `Test data` = RMSE_test
  ) %>% 
  # Convert results data frame to "tidy" data format i.e. long format, so that we
  # can ggplot it
  gather(type, RMSE, -num_coefficients)

ggplot(results, aes(x=num_coefficients, y=RMSE, col=type)) +
  geom_line() + 
  labs(x="# of coefficients", y="RMSE", col="Data used to evaluate \nperformance of fitted model")
```
```

With a large train and a small test set, we see an almost horizonal mirror image of the previously observed graph.  Critcally, the test data starts off at a very high RMSE but improves with each additional coefficient added.  In this scenario, we are essentially training on the entire dataset so our fitted model will have strong OOS predictability to the test data when a large number of coefficients are added.  

The training data's RMSE still is constant but at a much higher RMSE since we are using a larger dataset with more noise.



`

